==========================================
SLURM_JOB_ID = 8942488
SLURM_NODELIST = gpu-4-56
CUDA_VISIBLE_DEVICES = 0
==========================================
Unloading openmpi/4.1.5
Unloading slurm/23.02.6
Loading slurm/23.02.6
Loading openmpi/4.1.5
+ python GPU_check.py
cuda
+ env
+ grep SLURM
SLURM_JOB_USER=zhuosl
SLURM_TASKS_PER_NODE=1
SLURM_JOB_UID=1044122
SLURM_TASK_PID=2197564
SLURM_JOB_GPUS=1
SLURM_LOCALID=0
SLURM_SUBMIT_DIR=/home/zhuosl/VPML/DL_model
SLURMD_NODENAME=gpu-4-56
SLURM_JOB_START_TIME=1701062718
SLURM_NODE_ALIASES=(null)
SLURM_CLUSTER_NAME=farm
SLURM_JOB_END_TIME=1701188718
SLURM_CPUS_ON_NODE=32
SLURM_JOB_CPUS_PER_NODE=32
SLURM_GPUS_ON_NODE=1
SLURM_GTIDS=0
SLURM_JOB_PARTITION=gpu-a100-h
SLURM_JOB_NUM_NODES=1
SLURM_JOBID=8942488
SLURM_JOB_QOS=datalabgrp-gpu-a100-h-qos
SLURM_PROCID=0
SLURM_CPUS_PER_TASK=32
SLURM_TOPOLOGY_ADDR=gpu-4-56
SLURM_TOPOLOGY_ADDR_PATTERN=node
SLURM_SCRIPT_CONTEXT=prolog_task
SLURM_MEM_PER_NODE=64000
SLURM_WORKING_CLUSTER=farm:monitoring-ib:6817:9984:109
SLURM_ROOT=/share/apps/22.04/slurm/23.02.6
SLURM_NODELIST=gpu-4-56
SLURM_JOB_ACCOUNT=datalabgrp
SLURM_PRIO_PROCESS=0
SLURM_NNODES=1
SLURM_SUBMIT_HOST=farm
SLURM_JOB_ID=8942488
SLURM_NODEID=0
SLURM_CONF=/share/apps/22.04/slurm/23.02.6/etc/slurm.conf
SLURM_JOB_NAME=VP_ML
SLURM_JOB_GID=1044122
SLURM_JOB_NODELIST=gpu-4-56
+ scontrol show job 8942488
JobId=8942488 JobName=VP_ML
   UserId=zhuosl(1044122) GroupId=zhuosl(1044122) MCS_label=N/A
   Priority=9001 Nice=0 Account=datalabgrp QOS=datalabgrp-gpu-a100-h-qos
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:00:04 TimeLimit=1-11:00:00 TimeMin=N/A
   SubmitTime=2023-11-26T21:25:18 EligibleTime=2023-11-26T21:25:18
   AccrueTime=2023-11-26T21:25:18
   StartTime=2023-11-26T21:25:18 EndTime=2023-11-28T08:25:18 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-11-26T21:25:18 Scheduler=Main
   Partition=gpu-a100-h AllocNode:Sid=farm:3030342
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=gpu-4-56
   BatchHost=gpu-4-56
   NumNodes=1 NumCPUs=32 NumTasks=1 CPUs/Task=32 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=32,mem=62.50G,node=1,billing=32,gres/gpu=1
   AllocTRES=cpu=32,mem=62.50G,node=1,billing=32,gres/gpu=1
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=32 MinMemoryNode=62.50G MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/home/zhuosl/VPML/DL_model/GPU_check.sh
   WorkDir=/home/zhuosl/VPML/DL_model
   StdErr=/home/zhuosl/VPML/DL_model/GPUcheck.log
   StdIn=/dev/null
   StdOut=/home/zhuosl/VPML/DL_model/GPUcheck.log
   Power=
   TresPerNode=gres:gpu:1
   MailUser=zsliu@ucdavis.edu MailType=INVALID_DEPEND,BEGIN,END,FAIL,REQUEUE,STAGE_OUT
   

+ sstat --format JobID,MaxRSS,AveCPU -j 8942488
JobID            MaxRSS     AveCPU 
------------ ---------- ---------- 

############### Job 8942488 summary ###############
Name                : VP_ML
User                : zhuosl
Account             : datalabgrp
Partition           : gpu-a100-h
Nodes               : gpu-4-56
Cores               : 32
GPUs                : 1
State               : COMPLETED
ExitCode            : 0:0
Submit              : 2023-11-26T21:25:18
Start               : 2023-11-26T21:25:18
End                 : 2023-11-26T21:25:22
Reserved walltime   : 1-11:00:00
Used walltime       :   00:00:04
Used CPU time       :   00:00:04
% User (Computation): 35.89%
% System (I/O)      : 64.09%
Mem reserved        : 62.50G
Max Mem used        : 108.00K (gpu-4-56)
Max Disk Write      : 0.00  (gpu-4-56)
Max Disk Read       : 10.24K (gpu-4-56)
