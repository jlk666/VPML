{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLScript import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "# Total samples\n",
    "n_samples = 1980\n",
    "# Number of folds\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize KFold with 10 splits, shuffling for randomness\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Generate indices for each fold\n",
    "fold_indices = []\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(range(n_samples))):\n",
    "    fold_indices.append({\n",
    "        'fold': fold + 1,\n",
    "        'train_index': train_index,\n",
    "        'test_index': test_index\n",
    "    })\n",
    "fold_df = pd.DataFrame(fold_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score\n",
    "\n",
    "def SVM(X, Y, fold_df, save_results=True):\n",
    "    C = 0.88\n",
    "    kernel = 'rbf'\n",
    "    gamma = 0.005\n",
    "\n",
    "    svm_classifier = SVC(C=C, kernel=kernel, gamma=gamma, probability=True, random_state=42)\n",
    "\n",
    "    # Initialize lists to store metrics for each fold\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    all_predicted_probabilities = []\n",
    "    all_true_labels = []\n",
    "    time_per_fold = []\n",
    "    memory_per_fold = []\n",
    "\n",
    "    # Loop over each fold's indices from fold_df\n",
    "    for _, row in fold_df.iterrows():\n",
    "        train_index = row['train_index']\n",
    "        test_index = row['test_index']\n",
    "\n",
    "        # Split data into training and testing sets for the current fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # Track time and memory usage for the fold\n",
    "        start_time = time.time()\n",
    "        start_memory = psutil.Process().memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "        # Train the SVM classifier\n",
    "        svm_classifier.fit(X_train, Y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        Y_pred = svm_classifier.predict(X_test)\n",
    "        Y_pred_prob = svm_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # End time and memory tracking\n",
    "        end_time = time.time()\n",
    "        end_memory = psutil.Process().memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "        # Calculate time and memory usage\n",
    "        elapsed_time = end_time - start_time\n",
    "        memory_usage = end_memory - start_memory\n",
    "\n",
    "        time_per_fold.append(elapsed_time)\n",
    "        memory_per_fold.append(memory_usage)\n",
    "\n",
    "        # Collect metrics for this fold\n",
    "        accuracy_scores.append(accuracy_score(Y_test, Y_pred))\n",
    "        f1_scores.append(f1_score(Y_test, Y_pred))\n",
    "        precision_scores.append(precision_score(Y_test, Y_pred))\n",
    "        recall_scores.append(recall_score(Y_test, Y_pred))\n",
    "        all_predicted_probabilities.extend(Y_pred_prob)\n",
    "        all_true_labels.extend(Y_test)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric across all folds\n",
    "    results = {\n",
    "        'Accuracy': f\"{np.mean(accuracy_scores):.2f} ± {np.std(accuracy_scores):.2f}\",\n",
    "        'F1': f\"{np.mean(f1_scores):.2f} ± {np.std(f1_scores):.2f}\",\n",
    "        'Precision': f\"{np.mean(precision_scores):.2f} ± {np.std(precision_scores):.2f}\",\n",
    "        'Recall': f\"{np.mean(recall_scores):.2f} ± {np.std(recall_scores):.2f}\",\n",
    "        'Total Time (s)': f\"{np.sum(time_per_fold):.2f}\",\n",
    "        'Total Memory (MB)': f\"{np.sum(memory_per_fold):.2f}\"\n",
    "    }\n",
    "\n",
    "    # Calculate ROC curve and AUC score using the aggregated probabilities\n",
    "    fpr, tpr, _ = roc_curve(all_true_labels, all_predicted_probabilities)\n",
    "    auc_score = roc_auc_score(all_true_labels, all_predicted_probabilities)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Results of SVM:\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    # Optionally save results to a file\n",
    "    if save_results:\n",
    "        with open(\"svm_performance_results.txt\", \"w\") as results_file:\n",
    "            for metric, value in results.items():\n",
    "                results_file.write(f\"{metric}: {value}\\n\")\n",
    "\n",
    "    return results, auc_score, fpr, tpr\n",
    "\n",
    "\n",
    "\n",
    "def RF(X, Y, fold_df, save_results=True):\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    scoring_metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "    results = {metric.capitalize(): [] for metric in scoring_metrics}\n",
    "    \n",
    "    all_predicted_probabilities = []\n",
    "    all_true_labels = []\n",
    "    time_per_fold = []\n",
    "    memory_per_fold = []\n",
    "\n",
    "    # Loop through each fold in fold_df\n",
    "    for _, row in fold_df.iterrows():\n",
    "        train_index = row['train_index']\n",
    "        test_index = row['test_index']\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # Track time and memory usage\n",
    "        start_time = time.time()\n",
    "        start_memory = psutil.Process().memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "        rf_classifier.fit(X_train, Y_train)\n",
    "        Y_pred = rf_classifier.predict(X_test)\n",
    "        Y_pred_prob = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # End time and memory tracking\n",
    "        end_time = time.time()\n",
    "        end_memory = psutil.Process().memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "        elapsed_time = end_time - start_time\n",
    "        memory_usage = end_memory - start_memory\n",
    "\n",
    "        time_per_fold.append(elapsed_time)\n",
    "        memory_per_fold.append(memory_usage)\n",
    "\n",
    "        # Collect metrics for this fold\n",
    "        results['Accuracy'].append(accuracy_score(Y_test, Y_pred))\n",
    "        results['F1'].append(f1_score(Y_test, Y_pred, average='macro'))\n",
    "        results['Precision'].append(precision_score(Y_test, Y_pred, average='macro'))\n",
    "        results['Recall'].append(recall_score(Y_test, Y_pred, average='macro'))\n",
    "        \n",
    "        all_predicted_probabilities.extend(Y_pred_prob)\n",
    "        all_true_labels.extend(Y_test)\n",
    "    \n",
    "    # Calculate mean and std for each metric\n",
    "    metrics_results = {metric: f\"{np.mean(scores):.2f} ± {np.std(scores):.2f}\" for metric, scores in results.items()}\n",
    "    metrics_results['Total Time (s)'] = f\"{np.sum(time_per_fold):.2f} \"\n",
    "    metrics_results['Total Memory (MB)'] = f\"{np.sum(memory_per_fold):.2f} \"\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(all_true_labels, all_predicted_probabilities)\n",
    "    auc_score = roc_auc_score(all_true_labels, all_predicted_probabilities)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Results of Random Forest Classifier:\")\n",
    "    for metric, value in metrics_results.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    # Optionally save results\n",
    "    if save_results:\n",
    "        with open(\"rf_performance_results.txt\", \"w\") as results_file:\n",
    "            for metric, value in metrics_results.items():\n",
    "                results_file.write(f\"{metric}: {value}\\n\")\n",
    "\n",
    "    return metrics_results, auc_score, fpr, tpr\n",
    "\n",
    "def RF_explainer(X, Y, column_dict, csv_filename):\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_classifier.fit(X, Y)\n",
    "    feature_importances = rf_classifier.feature_importances_\n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "    sorted_feature_names = [column_dict[i + 1] for i in sorted_indices]\n",
    "    sorted_importance_values = feature_importances[sorted_indices]\n",
    "    \n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Feature Name', 'Importance Value'])\n",
    "        for feature_name, importance_value in zip(sorted_feature_names, sorted_importance_values):\n",
    "            writer.writerow([feature_name, importance_value])\n",
    "    \n",
    "    return sorted_feature_names, sorted_importance_values\n",
    "\n",
    "def KNN(X, Y, fold_df, save_results=True):\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    scoring_metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "    results = {metric.capitalize(): [] for metric in scoring_metrics}\n",
    "    \n",
    "    all_predicted_probabilities = []\n",
    "    all_true_labels = []\n",
    "    time_per_fold = []\n",
    "    memory_per_fold = []\n",
    "\n",
    "    for _, row in fold_df.iterrows():\n",
    "        train_index = row['train_index']\n",
    "        test_index = row['test_index']\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # Track time and memory usage\n",
    "        start_time = time.time()\n",
    "        start_memory = psutil.Process().memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "        knn_classifier.fit(X_train, Y_train)\n",
    "        Y_pred = knn_classifier.predict(X_test)\n",
    "        Y_pred_prob = knn_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # End time and memory tracking\n",
    "        end_time = time.time()\n",
    "        end_memory = psutil.Process().memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "        elapsed_time = end_time - start_time\n",
    "        memory_usage = end_memory - start_memory\n",
    "\n",
    "        time_per_fold.append(elapsed_time)\n",
    "        memory_per_fold.append(memory_usage)\n",
    "\n",
    "        # Collect metrics for this fold\n",
    "        results['Accuracy'].append(accuracy_score(Y_test, Y_pred))\n",
    "        results['F1'].append(f1_score(Y_test, Y_pred, average='macro'))\n",
    "        results['Precision'].append(precision_score(Y_test, Y_pred, average='macro'))\n",
    "        results['Recall'].append(recall_score(Y_test, Y_pred, average='macro'))\n",
    "        \n",
    "        all_predicted_probabilities.extend(Y_pred_prob)\n",
    "        all_true_labels.extend(Y_test)\n",
    "\n",
    "    # Calculate mean and std for each metric\n",
    "    metrics_results = {metric: f\"{np.mean(scores):.2f} ± {np.std(scores):.2f}\" for metric, scores in results.items()}\n",
    "    metrics_results['Total Time (s)'] = f\"{np.sum(time_per_fold):.2f} \"\n",
    "    metrics_results['Total Memory (MB)'] = f\"{np.sum(memory_per_fold):.2f} \"\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(all_true_labels, all_predicted_probabilities)\n",
    "    auc_score = roc_auc_score(all_true_labels, all_predicted_probabilities)\n",
    "\n",
    "    print(\"Results of K-Nearest Neighbors Classifier:\")\n",
    "    for metric, value in metrics_results.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    # Optionally save results\n",
    "    if save_results:\n",
    "        with open(\"knn_performance_results.txt\", \"w\") as results_file:\n",
    "            for metric, value in metrics_results.items():\n",
    "                results_file.write(f\"{metric}: {value}\\n\")\n",
    "\n",
    "    return metrics_results, auc_score, fpr, tpr\n",
    "\n",
    "def GaussianNB_(X, Y, fold_df, save_results=True):\n",
    "    gnb_classifier = GaussianNB()\n",
    "    scoring_metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "    results = {metric.capitalize(): [] for metric in scoring_metrics}\n",
    "    \n",
    "    all_predicted_probabilities = []\n",
    "    all_true_labels = []\n",
    "    time_per_fold = []\n",
    "    memory_per_fold = []\n",
    "\n",
    "    for _, row in fold_df.iterrows():\n",
    "        train_index = row['train_index']\n",
    "        test_index = row['test_index']\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # Track time and memory usage\n",
    "        start_time = time.time()\n",
    "        start_memory = psutil.Process().memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "        gnb_classifier.fit(X_train, Y_train)\n",
    "        Y_pred = gnb_classifier.predict(X_test)\n",
    "        Y_pred_prob = gnb_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # End time and memory tracking\n",
    "        end_time = time.time()\n",
    "        end_memory = psutil.Process().memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "        elapsed_time = end_time - start_time\n",
    "        memory_usage = end_memory - start_memory\n",
    "\n",
    "        time_per_fold.append(elapsed_time)\n",
    "        memory_per_fold.append(memory_usage)\n",
    "\n",
    "        # Collect metrics for this fold\n",
    "        results['Accuracy'].append(accuracy_score(Y_test, Y_pred))\n",
    "        results['F1'].append(f1_score(Y_test, Y_pred, average='macro'))\n",
    "        results['Precision'].append(precision_score(Y_test, Y_pred, average='macro'))\n",
    "        results['Recall'].append(recall_score(Y_test, Y_pred, average='macro'))\n",
    "        \n",
    "        all_predicted_probabilities.extend(Y_pred_prob)\n",
    "        all_true_labels.extend(Y_test)\n",
    "\n",
    "    # Calculate mean and std for each metric\n",
    "    metrics_results = {metric: f\"{np.mean(scores):.2f} ± {np.std(scores):.2f}\" for metric, scores in results.items()}\n",
    "    metrics_results['Total Time (s)'] = f\"{np.sum(time_per_fold):.2f} \"\n",
    "    metrics_results['Total Memory (MB)'] = f\"{np.sum(memory_per_fold):.2f} \"\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(all_true_labels, all_predicted_probabilities)\n",
    "    auc_score = roc_auc_score(all_true_labels, all_predicted_probabilities)\n",
    "\n",
    "    print(\"Results of Gaussian Naive Bayes Classifier:\")\n",
    "    for metric, value in metrics_results.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    # Optionally save results\n",
    "    if save_results:\n",
    "        with open(\"gaussian_nb_performance_results.txt\", \"w\") as results_file:\n",
    "            for metric, value in metrics_results.items():\n",
    "                results_file.write(f\"{metric}: {value}\\n\")\n",
    "\n",
    "    return metrics_results, auc_score, fpr, tpr\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "def SKLearnBoosting(X, Y, fold_df, save_results=True):\n",
    "    # Initialize the scikit-learn HistGradientBoostingClassifier\n",
    "    sk_boosting_classifier = HistGradientBoostingClassifier(random_state=42)\n",
    "    scoring_metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "    results = {metric.capitalize(): [] for metric in scoring_metrics}\n",
    "    \n",
    "    all_predicted_probabilities = []\n",
    "    all_true_labels = []\n",
    "    time_per_fold = []\n",
    "    memory_per_fold = []\n",
    "\n",
    "    # Loop through each fold in fold_df\n",
    "    for _, row in fold_df.iterrows():\n",
    "        train_index = row['train_index']\n",
    "        test_index = row['test_index']\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # Track time and memory usage\n",
    "        start_time = time.time()\n",
    "        start_memory = psutil.Process().memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "        sk_boosting_classifier.fit(X_train, Y_train)\n",
    "        Y_pred = sk_boosting_classifier.predict(X_test)\n",
    "        Y_pred_prob = sk_boosting_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # End time and memory tracking\n",
    "        end_time = time.time()\n",
    "        end_memory = psutil.Process().memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "        elapsed_time = end_time - start_time\n",
    "        memory_usage = end_memory - start_memory\n",
    "\n",
    "        time_per_fold.append(elapsed_time)\n",
    "        memory_per_fold.append(memory_usage)\n",
    "\n",
    "        # Collect metrics for this fold\n",
    "        results['Accuracy'].append(accuracy_score(Y_test, Y_pred))\n",
    "        results['F1'].append(f1_score(Y_test, Y_pred, average='macro'))\n",
    "        results['Precision'].append(precision_score(Y_test, Y_pred, average='macro'))\n",
    "        results['Recall'].append(recall_score(Y_test, Y_pred, average='macro'))\n",
    "        \n",
    "        all_predicted_probabilities.extend(Y_pred_prob)\n",
    "        all_true_labels.extend(Y_test)\n",
    "\n",
    "    # Calculate mean and std for each metric\n",
    "    metrics_results = {metric: f\"{np.mean(scores):.2f} ± {np.std(scores):.2f}\" for metric, scores in results.items()}\n",
    "    metrics_results['Total Time (s)'] = f\"{np.sum(time_per_fold):.2f} \"\n",
    "    metrics_results['Total Memory (MB)'] = f\"{np.sum(memory_per_fold):.2f} \"\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(all_true_labels, all_predicted_probabilities)\n",
    "    auc_score = roc_auc_score(all_true_labels, all_predicted_probabilities)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Results of HistGradientBoosting Classifier:\")\n",
    "    for metric, value in metrics_results.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    # Optionally save results\n",
    "    if save_results:\n",
    "        with open(\"sk_boosting_performance_results.txt\", \"w\") as results_file:\n",
    "            for metric, value in metrics_results.items():\n",
    "                results_file.write(f\"{metric}: {value}\\n\")\n",
    "\n",
    "    return metrics_results, auc_score, fpr, tpr\n",
    "\n",
    "def plot_roc_curves(models_fpr_tpr_auc, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot each model's ROC curve\n",
    "    for model_name, (fpr, tpr, auc_score) in models_fpr_tpr_auc.items():\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.2f})')\n",
    "    \n",
    "    # Plot a dashed diagonal line for random guessing\n",
    "    plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\", label=\"Random Guessing\")\n",
    "    \n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Genome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y, column_dict, genome_id = process_genome_matrix('core_genome.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_svm_c, auc_score_svm_c, fpr_svm_c, tpr_svm_c = SVM(X, Y, fold_df, save_results=False)\n",
    "results_rf_c, auc_score_rf_c, fpr_rf_c, tpr_rf_c = RF(X, Y, fold_df)\n",
    "results_knn_c, auc_score_knn_c, fpr_knn_c, tpr_knn_c = KNN(X, Y, fold_df)\n",
    "results_XGBoost_c, auc_score4_XGBoost_c, fpr4_XGBoost_c, tpr4_XGBoost_c  = SKLearnBoosting(X, Y, fold_df, save_results=True)\n",
    "results_NB_c, auc_score4_NB_c, fpr_NB_c, tpr_NB_c = GaussianNB_(X, Y, fold_df, save_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_fpr_tpr_auc = {\n",
    "    'Support Vector Machine': (fpr_svm_c, tpr_svm_c, auc_score_svm_c),\n",
    "    'K-Nearest Neighbors': (fpr_knn_c, tpr_knn_c, auc_score_knn_c),\n",
    "    'Random Forest': (fpr_rf_c, tpr_rf_c, auc_score_rf_c),\n",
    "    'Gradient Boosting Trees': (fpr4_XGBoost_c, tpr4_XGBoost_c, auc_score4_XGBoost_c),\n",
    "    'Naive Bayes': (fpr_NB_c, tpr_NB_c, auc_score4_NB_c)\n",
    "}\n",
    "\n",
    "plot_roc_curves(models_fpr_tpr_auc, 'Model performance based on core pangenome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results into a dictionary with the desired order\n",
    "models = ['Support Vector Machine', 'K-Nearest Neighbors', 'Random Forest', 'Gradient Boosting Trees', 'Naive Bayes']\n",
    "results_ordered = [results_svm_c, results_knn_c, results_rf_c, results_XGBoost_c, results_NB_c]\n",
    "\n",
    "# Metrics to extract\n",
    "metrics = ['Accuracy', 'F1', 'Precision', 'Recall']\n",
    "\n",
    "# Parse results into means and stds\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for model_results in results_ordered:\n",
    "    model_means = []\n",
    "    model_stds = []\n",
    "    for metric in metrics:\n",
    "        mean, std = map(float, model_results[metric].split(' ± '))\n",
    "        model_means.append(mean)\n",
    "        model_stds.append(std)\n",
    "    means.append(model_means)\n",
    "    stds.append(model_stds)\n",
    "\n",
    "# Convert to numpy arrays for easier plotting\n",
    "means = np.array(means)\n",
    "stds = np.array(stds)\n",
    "\n",
    "# Define colors consistent with the ROC curve\n",
    "colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "\n",
    "# Bar graph parameters\n",
    "x = np.arange(len(metrics))  # Metric indices\n",
    "width = 0.15  # Bar width\n",
    "\n",
    "# Plot bars for each model\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, (model, model_means, model_stds, color) in enumerate(zip(models, means, stds, colors)):\n",
    "    ax.bar(\n",
    "        x + i * width, \n",
    "        model_means, \n",
    "        width, \n",
    "        label=f\"{model}\", \n",
    "        color=color, \n",
    "        yerr=model_stds, \n",
    "        capsize=5\n",
    "    )\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('ROC curves for models based on core pangenome')\n",
    "ax.set_xticks(x + width * (len(models) - 1) / 2)\n",
    "ax.set_xticklabels(metrics)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### core soft pangenome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y, column_dict, genome_id = process_genome_matrix('core_soft_genome.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_svm_c, auc_score_svm_c, fpr_svm_c, tpr_svm_c = SVM(X, Y, fold_df, save_results=False)\n",
    "results_rf_c, auc_score_rf_c, fpr_rf_c, tpr_rf_c = RF(X, Y, fold_df)\n",
    "results_knn_c, auc_score_knn_c, fpr_knn_c, tpr_knn_c = KNN(X, Y, fold_df)\n",
    "results_XGBoost_c, auc_score4_XGBoost_c, fpr4_XGBoost_c, tpr4_XGBoost_c  = SKLearnBoosting(X, Y, fold_df, save_results=True)\n",
    "results_NB_c, auc_score4_NB_c, fpr_NB_c, tpr_NB_c = GaussianNB_(X, Y, fold_df, save_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_fpr_tpr_auc = {\n",
    "    'Support Vector Machine': (fpr_svm_c, tpr_svm_c, auc_score_svm_c),\n",
    "    'K-Nearest Neighbors': (fpr_knn_c, tpr_knn_c, auc_score_knn_c),\n",
    "    'Random Forest': (fpr_rf_c, tpr_rf_c, auc_score_rf_c),\n",
    "    'Gradient Boosting Trees': (fpr4_XGBoost_c, tpr4_XGBoost_c, auc_score4_XGBoost_c),\n",
    "    'Naive Bayes': (fpr_NB_c, tpr_NB_c, auc_score4_NB_c)\n",
    "}\n",
    "\n",
    "plot_roc_curves(models_fpr_tpr_auc, 'Model performance based on soft core pangenome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results into a dictionary with the desired order\n",
    "models = ['Support Vector Machine', 'K-Nearest Neighbors', 'Random Forest', 'Gradient Boosting Trees', 'Naive Bayes']\n",
    "results_ordered = [results_svm_c, results_knn_c, results_rf_c, results_XGBoost_c, results_NB_c]\n",
    "\n",
    "# Metrics to extract\n",
    "metrics = ['Accuracy', 'F1', 'Precision', 'Recall']\n",
    "\n",
    "# Parse results into means and stds\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for model_results in results_ordered:\n",
    "    model_means = []\n",
    "    model_stds = []\n",
    "    for metric in metrics:\n",
    "        mean, std = map(float, model_results[metric].split(' ± '))\n",
    "        model_means.append(mean)\n",
    "        model_stds.append(std)\n",
    "    means.append(model_means)\n",
    "    stds.append(model_stds)\n",
    "\n",
    "# Convert to numpy arrays for easier plotting\n",
    "means = np.array(means)\n",
    "stds = np.array(stds)\n",
    "\n",
    "# Define colors consistent with the ROC curve\n",
    "colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "\n",
    "# Bar graph parameters\n",
    "x = np.arange(len(metrics))  # Metric indices\n",
    "width = 0.15  # Bar width\n",
    "\n",
    "# Plot bars for each model\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, (model, model_means, model_stds, color) in enumerate(zip(models, means, stds, colors)):\n",
    "    ax.bar(\n",
    "        x + i * width, \n",
    "        model_means, \n",
    "        width, \n",
    "        label=f\"{model}\", \n",
    "        color=color, \n",
    "        yerr=model_stds, \n",
    "        capsize=5\n",
    "    )\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('ROC curves for models based on soft core pangenome')\n",
    "ax.set_xticks(x + width * (len(models) - 1) / 2)\n",
    "ax.set_xticklabels(metrics)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### core + shell pangenome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y, column_dict, genome_id = process_genome_matrix('Core_shell_genome.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_svm_c, auc_score_svm_c, fpr_svm_c, tpr_svm_c = SVM(X, Y, fold_df, save_results=False)\n",
    "results_rf_c, auc_score_rf_c, fpr_rf_c, tpr_rf_c = RF(X, Y, fold_df)\n",
    "results_knn_c, auc_score_knn_c, fpr_knn_c, tpr_knn_c = KNN(X, Y, fold_df)\n",
    "results_XGBoost_c, auc_score4_XGBoost_c, fpr4_XGBoost_c, tpr4_XGBoost_c  = SKLearnBoosting(X, Y, fold_df, save_results=True)\n",
    "results_NB_c, auc_score4_NB_c, fpr_NB_c, tpr_NB_c = GaussianNB_(X, Y, fold_df, save_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_fpr_tpr_auc = {\n",
    "    'Support Vector Machine': (fpr_svm_c, tpr_svm_c, auc_score_svm_c),\n",
    "    'K-Nearest Neighbors': (fpr_knn_c, tpr_knn_c, auc_score_knn_c),\n",
    "    'Random Forest': (fpr_rf_c, tpr_rf_c, auc_score_rf_c),\n",
    "    'Gradient Boosting Trees': (fpr4_XGBoost_c, tpr4_XGBoost_c, auc_score4_XGBoost_c),\n",
    "    'Naive Bayes': (fpr_NB_c, tpr_NB_c, auc_score4_NB_c)\n",
    "}\n",
    "\n",
    "plot_roc_curves(models_fpr_tpr_auc, 'Model performance based on soft core + shell pangenome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results into a dictionary with the desired order\n",
    "models = ['Support Vector Machine', 'K-Nearest Neighbors', 'Random Forest', 'Gradient Boosting Trees', 'Naive Bayes']\n",
    "results_ordered = [results_svm_c, results_knn_c, results_rf_c, results_XGBoost_c, results_NB_c]\n",
    "\n",
    "# Metrics to extract\n",
    "metrics = ['Accuracy', 'F1', 'Precision', 'Recall']\n",
    "\n",
    "# Parse results into means and stds\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for model_results in results_ordered:\n",
    "    model_means = []\n",
    "    model_stds = []\n",
    "    for metric in metrics:\n",
    "        mean, std = map(float, model_results[metric].split(' ± '))\n",
    "        model_means.append(mean)\n",
    "        model_stds.append(std)\n",
    "    means.append(model_means)\n",
    "    stds.append(model_stds)\n",
    "\n",
    "# Convert to numpy arrays for easier plotting\n",
    "means = np.array(means)\n",
    "stds = np.array(stds)\n",
    "\n",
    "# Define colors consistent with the ROC curve\n",
    "colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "\n",
    "# Bar graph parameters\n",
    "x = np.arange(len(metrics))  # Metric indices\n",
    "width = 0.15  # Bar width\n",
    "\n",
    "# Plot bars for each model\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, (model, model_means, model_stds, color) in enumerate(zip(models, means, stds, colors)):\n",
    "    ax.bar(\n",
    "        x + i * width, \n",
    "        model_means, \n",
    "        width, \n",
    "        label=f\"{model}\", \n",
    "        color=color, \n",
    "        yerr=model_stds, \n",
    "        capsize=5\n",
    "    )\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('ROC curves for models based on soft core + shell pangenome')\n",
    "ax.set_xticks(x + width * (len(models) - 1) / 2)\n",
    "ax.set_xticklabels(metrics)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### whole pangenome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y, column_dict, genome_id = process_genome_matrix('genome_matrix_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_svm, auc_score_svm, fpr_svm, tpr_svm = SVM(X, Y, fold_df, save_results=False)\n",
    "results_rf, auc_score_rf, fpr_rf, tpr_rf = RF(X, Y, fold_df)\n",
    "results_knn, auc_score_knn, fpr_knn, tpr_knn = KNN(X, Y, fold_df)\n",
    "results_XGBoost, auc_score4_XGBoost, fpr4_XGBoost, tpr4_XGBoost  = SKLearnBoosting(X, Y, fold_df, save_results=True)\n",
    "results_NB, auc_score4_NB, fpr_NB, tpr_NB = GaussianNB_(X, Y, fold_df, save_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_fpr_tpr_auc = {\n",
    "    'Support Vector Machine': (fpr_svm, tpr_svm, auc_score_svm),\n",
    "    'K-Nearest Neighbors': (fpr_knn, tpr_knn, auc_score_knn),\n",
    "    'Random Forest': (fpr_rf, tpr_rf, auc_score_rf),\n",
    "    'Gradient Boosting Trees': (fpr4_XGBoost, tpr4_XGBoost, auc_score4_XGBoost),\n",
    "    'Naive Bayes': (fpr_NB, tpr_NB, auc_score4_NB)\n",
    "}\n",
    "\n",
    "plot_roc_curves(models_fpr_tpr_auc, 'Model performance based on full pangenome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bars for each model\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, (model, model_means, model_stds, color) in enumerate(zip(models, means, stds, colors)):\n",
    "    ax.bar(\n",
    "        x + i * width, \n",
    "        model_means, \n",
    "        width, \n",
    "        label=f\"{model}\", \n",
    "        color=color, \n",
    "        yerr=model_stds, \n",
    "        capsize=5\n",
    "    )\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('ROC curves for models based on selected pangenome')\n",
    "ax.set_xticks(x + width * (len(models) - 1) / 2)\n",
    "ax.set_xticklabels(metrics)\n",
    "\n",
    "# Move legend outside the main figure\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0.0, 1.0)\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust rect to leave space on the right for the legend\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout(rect=[0, 0, 1.2, 1])  # Adjust rect to leave space on the right for the legend\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
